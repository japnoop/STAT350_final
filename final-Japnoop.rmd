---
title: "Real Estate Analysis"
author: "Japnoop Grewal"
date: "November 20, 2020"
output: html_document
---


```{r}
library(tidyverse)
library(caret)
library(car)
library(olsrr)
library(ggmap)
library(sf)
library(mapview)
```

#Reading/importing the data
```{r}
RealEstate=read.csv('Real estate.csv')
```

#Renaming the data
```{r}
RealEstate=rename(RealEstate,c('HousePriceOfUnitArea'='Y.house.price.of.unit.area','TransactionDate'='X1.transaction.date','HouseAge'='X2.house.age','DistanceToMRT'='X3.distance.to.the.nearest.MRT.station','NumberOfConStores'='X4.number.of.convenience.stores','Latitude'='X5.latitude','Longitude'='X6.longitude'))
```

#Adding new datapoints
```{r}
#j<-c(415,2013,20,4548,0,24.94846,121,4955,70)
#k<-c(416,2013,10,40,7,24.9676,121.5408,15)
#RealEstate<-rbind(RealEstate,j,k)
```

#Introducing a new explanatory variable
```{r}
HousePriceDueToMRT=RealEstate$HousePriceOfUnitArea/RealEstate$DistanceToMRT
#I introduced a new variable which shows the price of the house of unit area due to the how far it is to the MRT Station

RealEstate = cbind(RealEstate,HousePriceDueToMRT)
#binding the new data point to the dataset
```

#Map View
```{r}
longitude<-RealEstate$Longitude
latitude<-RealEstate$Latitude
priceToMRT<-RealEstate$HousePriceDueToMRT
price<-RealEstate$HousePriceOfUnitArea

locations<-cbind(longitude,latitude)
locations<-as.tibble(locations)

locations_price<-cbind(locations,price,priceToMRT)
locations_sf<-st_as_sf(locations_price,coords=c("longitude","latitude"),crs=4326)

#Mapview of real estate by price
mapview(locations_sf,zcol= "price", at = seq(0,120,60),legend=TRUE)

#Mapview of real estate by price to Distance to MRT ratio
mapview(locations_sf,zcol= "priceToMRT",legend=TRUE)
```

#Cross Validation
```{r}
set.seed(10)
samplesize=ceiling(0.8 * nrow(RealEstate))

TrainSamples=sample(seq_len(nrow(RealEstate)),samplesize)
TrainSamples=sort(TrainSamples)

Train=RealEstate[TrainSamples, ]

Test=RealEstate[-TrainSamples, ]

TrainLM=lm(HousePriceOfUnitArea ~ TransactionDate + HouseAge + DistanceToMRT + NumberOfConStores + Latitude + Longitude + HousePriceDueToMRT, data=RealEstate)

summary(TrainLM)

#In the summary of the TrainLM we see that longitude is the only variable with a high p-value. This high p-value is significant because it shows that we should get rid of the longitude variable. 
```

#Model without longitude
```{r}
model.TrainLM=lm(HousePriceOfUnitArea ~ TransactionDate + HouseAge + DistanceToMRT + NumberOfConStores + Latitude + HousePriceDueToMRT, data=Train)

summary(model.TrainLM)

#This model is better than the other one because all the p-values are relatively small.
```

#Residual Analysis
```{r}
plot(model.TrainLM)
plot(model.TrainLM, which = 4)

#The Residuals vs Fitted plot shows that the spread seems to be increasing and points 114,271,313 might be outliers.

#The points fall along the reference line for the Normal Q-Q plot so the normality assumption is reasonable.

#The Residuals vs Leverage plot shows that points 20 and 276 have Cook's distance of more than 0.5 so they are influential points.

#The Cook's Distance plot shows that 271 has a pretty high value of about 0.26 but it is not more than 0.5 so it is not influential, but points 20 and 276 are. 
```

#Checking for collinearity and multicollinearity
```{r}
vif(model.TrainLM)

#The VIF values are all smaller than 5 so there is no sign of multicollinearity. 
```

#Transformaion of the model
```{r}
ln.TrainLM=lm(log(HousePriceOfUnitArea) ~ TransactionDate + HouseAge + DistanceToMRT + NumberOfConStores + Latitude + HousePriceDueToMRT, data=Train)

summary(ln.TrainLM)

#The log transformation makes it so the residual standard error goes down and the r and r-squared both increase. This all means that the model fits better. 
```

#Looking for outliers/leverage points
```{r}
plot(ln.TrainLM)
influencePlot(ln.TrainLM)

#We remove points 114 and 149 because they have a studentized residual of more than 3 so they are considered outliers.  All the Cook's distances are less than 0.5 so there are no influential points.
```

#Getting rid of the outliers
```{r}
Train=Train[-c(114,149),]

#Got rid of the 2 points because the skewed the data too much
```

#Model after removing all the ouliers
```{r}
ln.TrainLM2=lm(log(HousePriceOfUnitArea) ~ TransactionDate + HouseAge + DistanceToMRT + NumberOfConStores + Latitude + HousePriceDueToMRT, data=Train)

summary(ln.TrainLM2)
```

#Variable Selection
```{r}
ols_step_both_p(ln.TrainLM2)
#The stepwise regression added all the variables and got rid of none so that means that all the variables in the model are significant. 
```

#Analysis of the predicted model
```{r}
preds=predict(ln.TrainLM2,Test)
plot(Test$HousePriceOfUnitArea,preds)

R2(preds,Test$HousePriceOfUnitArea)
RMSE(preds,Test$HousePriceOfUnitArea)
MAE(preds,Test$HousePriceOfUnitArea)
```
